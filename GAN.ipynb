{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input image dimensions\n",
    "#Large images take too much time and resources.\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genrator takes in a latent vector(random noise) of dimension(100,)\n",
    "# and outputs an image of size(28,28,1)\n",
    "# Defining the generator architecture\n",
    "def build_generator():\n",
    "\n",
    "    noise_shape = (100,) #1D array of size 100 (latent vector / noise)\n",
    "\n",
    "    #Define your generator network    \n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    gen_img = model(noise)    #Generated image\n",
    "\n",
    "    return Model(noise, gen_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the discrinator takes in image of dimensions(28,28,1) and outputs\n",
    "# the probability of the image being real\n",
    "# defining the discriminator architecture\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "def train(epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    import gzip\n",
    "    import sys\n",
    "    import pickle\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    if sys.version_info < (3,):\n",
    "        data = pickle.load(f)\n",
    "    else:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    # (x_train, _), (x_test, _) = data\n",
    "    (X_train, _), (_, _) = data\n",
    "    # we are only taking in the actual images and not the other info such as class labels\n",
    "\n",
    "    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "    #Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n",
    "    X_train = np.expand_dims(X_train, axis=3) \n",
    "\n",
    "    half_batch = int(batch_size / 2)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # train the discriminator\n",
    "        # Select a random half batch of real images\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        real_imgs = X_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "        # Generate a half batch of fake images\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator on real and fake images, separately\n",
    "        #Research showed that separate training is more effective. \n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "\n",
    "        #take average loss from real and fake images. \n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
    "\n",
    "\n",
    "        #And within the same loop we train our Generator, by setting the input noise and\n",
    "        #ultimately training the Generator to have the Discriminator label its samples as valid\n",
    "        #by specifying the gradient loss.\n",
    "        \n",
    "        # Training the generator\n",
    "        \n",
    "        #Create noise vectors as input for generator. \n",
    "        #Create as many noise vectors as defined by the batch size. \n",
    "        #Based on normal distribution. Output will be of size (batch size, 100)\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100)) \n",
    "\n",
    "        # The generator wants the discriminator to label the generated samples\n",
    "        # as valid (ones)\n",
    "        #This is where the genrator is trying to trick discriminator into believing\n",
    "        #the generated image is true (hence value of 1 for y)\n",
    "        valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
    "\n",
    "        # Generator is part of combined where it got directly linked with the discriminator\n",
    "        # Train the generator with noise as x and 1 as y. \n",
    "        # Again, 1 as the output as it is adversarial and if generator did a great\n",
    "        #job of folling the discriminator then the output would be 1 (true)\n",
    "        g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "\n",
    "        #Additionally, in order for us to keep track of our training process, we print the\n",
    "        #progress and save the sample image output depending on the epoch interval specified.  \n",
    "        # Plot the progress\n",
    "        \n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when the specific sample_interval is hit, we call the\n",
    "#sample_image function. Which looks as follows.\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_9 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " leaky_re_lu_45 (LeakyReLU)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu_46 (LeakyReLU)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 533505 (2.04 MB)\n",
      "Trainable params: 533505 (2.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_66 (Dense)            (None, 256)               25856     \n",
      "                                                                 \n",
      " leaky_re_lu_47 (LeakyReLU)  (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_48 (LeakyReLU)  (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " leaky_re_lu_49 (LeakyReLU)  (None, 1024)              0         \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 1024)              4096      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 784)               803600    \n",
      "                                                                 \n",
      " reshape_9 (Reshape)         (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1493520 (5.70 MB)\n",
      "Trainable params: 1489936 (5.68 MB)\n",
      "Non-trainable params: 3584 (14.00 KB)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "0 [D loss: 0.865279, acc.: 18.75%] [G loss: 0.667992]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1 [D loss: 0.400960, acc.: 84.38%] [G loss: 0.662730]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "2 [D loss: 0.367567, acc.: 75.00%] [G loss: 0.704426]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "3 [D loss: 0.325281, acc.: 87.50%] [G loss: 0.744956]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "4 [D loss: 0.307062, acc.: 96.88%] [G loss: 0.811332]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "5 [D loss: 0.295094, acc.: 87.50%] [G loss: 0.959485]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "6 [D loss: 0.256390, acc.: 100.00%] [G loss: 1.043985]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "7 [D loss: 0.223097, acc.: 96.88%] [G loss: 1.180781]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "8 [D loss: 0.203613, acc.: 100.00%] [G loss: 1.360986]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "9 [D loss: 0.177544, acc.: 100.00%] [G loss: 1.429631]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "10 [D loss: 0.148744, acc.: 100.00%] [G loss: 1.627245]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "11 [D loss: 0.131527, acc.: 100.00%] [G loss: 1.748807]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "12 [D loss: 0.115178, acc.: 100.00%] [G loss: 1.931477]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "13 [D loss: 0.115455, acc.: 100.00%] [G loss: 2.066760]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "14 [D loss: 0.085056, acc.: 100.00%] [G loss: 2.122959]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "15 [D loss: 0.104157, acc.: 100.00%] [G loss: 2.230516]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "16 [D loss: 0.067888, acc.: 100.00%] [G loss: 2.239383]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "17 [D loss: 0.069129, acc.: 100.00%] [G loss: 2.427414]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "18 [D loss: 0.057956, acc.: 100.00%] [G loss: 2.565116]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "19 [D loss: 0.053750, acc.: 100.00%] [G loss: 2.565611]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "20 [D loss: 0.052315, acc.: 100.00%] [G loss: 2.530819]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "21 [D loss: 0.054639, acc.: 100.00%] [G loss: 2.675784]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "22 [D loss: 0.044913, acc.: 100.00%] [G loss: 2.684560]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "23 [D loss: 0.046621, acc.: 100.00%] [G loss: 2.819365]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "24 [D loss: 0.033850, acc.: 100.00%] [G loss: 2.828117]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "25 [D loss: 0.033300, acc.: 100.00%] [G loss: 2.898242]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "26 [D loss: 0.033499, acc.: 100.00%] [G loss: 2.865492]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "27 [D loss: 0.047106, acc.: 100.00%] [G loss: 2.938137]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "28 [D loss: 0.047914, acc.: 100.00%] [G loss: 3.229178]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "29 [D loss: 0.030305, acc.: 100.00%] [G loss: 3.118669]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "30 [D loss: 0.032623, acc.: 100.00%] [G loss: 3.114267]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "31 [D loss: 0.034088, acc.: 100.00%] [G loss: 3.272537]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "32 [D loss: 0.022959, acc.: 100.00%] [G loss: 3.314299]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "33 [D loss: 0.027777, acc.: 100.00%] [G loss: 3.410793]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "34 [D loss: 0.021849, acc.: 100.00%] [G loss: 3.247093]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "35 [D loss: 0.026520, acc.: 100.00%] [G loss: 3.283904]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "36 [D loss: 0.036952, acc.: 100.00%] [G loss: 3.386707]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "37 [D loss: 0.028601, acc.: 100.00%] [G loss: 3.377110]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "38 [D loss: 0.018714, acc.: 100.00%] [G loss: 3.468369]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "39 [D loss: 0.022720, acc.: 100.00%] [G loss: 3.628615]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "40 [D loss: 0.030941, acc.: 100.00%] [G loss: 3.526435]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "41 [D loss: 0.018326, acc.: 100.00%] [G loss: 3.701407]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "42 [D loss: 0.023023, acc.: 100.00%] [G loss: 3.631542]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "43 [D loss: 0.018707, acc.: 100.00%] [G loss: 3.541535]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "44 [D loss: 0.024375, acc.: 100.00%] [G loss: 3.744599]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "45 [D loss: 0.018023, acc.: 100.00%] [G loss: 3.652969]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "46 [D loss: 0.017049, acc.: 100.00%] [G loss: 3.546398]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "47 [D loss: 0.015004, acc.: 100.00%] [G loss: 3.736498]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "48 [D loss: 0.015494, acc.: 100.00%] [G loss: 3.616281]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "49 [D loss: 0.016234, acc.: 100.00%] [G loss: 3.869980]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "50 [D loss: 0.027452, acc.: 100.00%] [G loss: 3.838958]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "51 [D loss: 0.024492, acc.: 100.00%] [G loss: 3.829556]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "52 [D loss: 0.018563, acc.: 100.00%] [G loss: 3.689033]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "53 [D loss: 0.011433, acc.: 100.00%] [G loss: 3.923108]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "54 [D loss: 0.013404, acc.: 100.00%] [G loss: 3.825503]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "55 [D loss: 0.016507, acc.: 100.00%] [G loss: 3.810993]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "56 [D loss: 0.017892, acc.: 100.00%] [G loss: 3.754122]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "57 [D loss: 0.011956, acc.: 100.00%] [G loss: 3.791831]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "58 [D loss: 0.011686, acc.: 100.00%] [G loss: 3.779138]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "59 [D loss: 0.012096, acc.: 100.00%] [G loss: 3.812642]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "60 [D loss: 0.017261, acc.: 100.00%] [G loss: 3.746729]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "61 [D loss: 0.014572, acc.: 100.00%] [G loss: 3.731982]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "62 [D loss: 0.012609, acc.: 100.00%] [G loss: 3.804663]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "63 [D loss: 0.015616, acc.: 100.00%] [G loss: 3.684752]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "64 [D loss: 0.018964, acc.: 100.00%] [G loss: 3.825015]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "65 [D loss: 0.013211, acc.: 100.00%] [G loss: 3.837918]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "66 [D loss: 0.013095, acc.: 100.00%] [G loss: 3.773007]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "67 [D loss: 0.027656, acc.: 100.00%] [G loss: 3.909801]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "68 [D loss: 0.024070, acc.: 100.00%] [G loss: 4.008124]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "69 [D loss: 0.016188, acc.: 100.00%] [G loss: 3.989754]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "70 [D loss: 0.010364, acc.: 100.00%] [G loss: 4.130890]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "71 [D loss: 0.012276, acc.: 100.00%] [G loss: 4.050603]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "72 [D loss: 0.015304, acc.: 100.00%] [G loss: 4.082088]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "73 [D loss: 0.012272, acc.: 100.00%] [G loss: 3.950321]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "74 [D loss: 0.018677, acc.: 100.00%] [G loss: 4.187895]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "75 [D loss: 0.021463, acc.: 100.00%] [G loss: 4.035341]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "76 [D loss: 0.010871, acc.: 100.00%] [G loss: 4.098967]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "77 [D loss: 0.015924, acc.: 100.00%] [G loss: 4.104388]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "78 [D loss: 0.014982, acc.: 100.00%] [G loss: 4.264899]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "79 [D loss: 0.017433, acc.: 100.00%] [G loss: 4.250749]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "80 [D loss: 0.012219, acc.: 100.00%] [G loss: 4.209446]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "81 [D loss: 0.009072, acc.: 100.00%] [G loss: 4.158475]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "82 [D loss: 0.022587, acc.: 100.00%] [G loss: 4.084013]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "83 [D loss: 0.020186, acc.: 100.00%] [G loss: 4.201495]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "84 [D loss: 0.016509, acc.: 100.00%] [G loss: 4.106988]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "85 [D loss: 0.021291, acc.: 100.00%] [G loss: 4.130234]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "86 [D loss: 0.012355, acc.: 100.00%] [G loss: 4.125382]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "87 [D loss: 0.013458, acc.: 100.00%] [G loss: 4.027226]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "88 [D loss: 0.016420, acc.: 100.00%] [G loss: 4.242230]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "89 [D loss: 0.011925, acc.: 100.00%] [G loss: 4.250567]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "90 [D loss: 0.012043, acc.: 100.00%] [G loss: 4.345074]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "91 [D loss: 0.032796, acc.: 100.00%] [G loss: 4.293685]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "92 [D loss: 0.010661, acc.: 100.00%] [G loss: 4.553698]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "93 [D loss: 0.011947, acc.: 100.00%] [G loss: 4.440731]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "94 [D loss: 0.013762, acc.: 100.00%] [G loss: 4.410119]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "95 [D loss: 0.012476, acc.: 100.00%] [G loss: 4.315018]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "96 [D loss: 0.013074, acc.: 100.00%] [G loss: 4.238476]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "97 [D loss: 0.015708, acc.: 100.00%] [G loss: 4.340779]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "98 [D loss: 0.020963, acc.: 100.00%] [G loss: 4.497114]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "99 [D loss: 0.016291, acc.: 100.00%] [G loss: 4.433547]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "100 [D loss: 0.014023, acc.: 100.00%] [G loss: 4.576668]\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "# Defining other parameters\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
    "\n",
    "# Build and compile the discriminator first. \n",
    "#Generator will be trained as part of the combined model, later.\n",
    " \n",
    "#pick the loss function and the type of metric to keep track.                 \n",
    "#Binary cross entropy as we are doing prediction and it is a better\n",
    "#loss function compared to MSE or other. \n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "#build and compile our Discriminator, pick the loss function\n",
    "\n",
    "#SInce we are only generating (faking) images, let us not track any metrics.\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "##This builds the Generator and defines the input noise. \n",
    "#In a GAN the Generator network takes noise z as an input to produce its images.  \n",
    "z = Input(shape=(100,))   #Our random input to the generator\n",
    "img = generator(z)\n",
    "\n",
    "#This ensures that when we combine our networks we only train the Generator.\n",
    "#While generator training we do not want discriminator weights to be adjusted. \n",
    "#This Doesn't affect the above descriminator training.     \n",
    "discriminator.trainable = False  \n",
    "\n",
    "#This specifies that our Discriminator will take the images generated by our Generator\n",
    "#and true dataset and set its output to a parameter called valid, which will indicate\n",
    "#whether the input is real or not.  \n",
    "valid = discriminator(img)  #Validity check on the generated image\n",
    "\n",
    "#Here we combined the models and also set our loss function and optimizer. \n",
    "#Again, we are only training the generator here. \n",
    "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity\n",
    "\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "train(epochs=5000, batch_size=32, save_interval=10)\n",
    "\n",
    "#Save model for future use to generate fake images\n",
    "#Not tested yet... make sure right model is being saved..\n",
    "#Compare with GAN4\n",
    "\n",
    "generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n",
    "#Change epochs back to 30K\n",
    "                \n",
    "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
    "#indicates the number of training samples per backward/forward propagation, and the\n",
    "#sample_interval specifies after how many epochs we call our sample_image function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 151ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prediction from a saved model file\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load model\n",
    "model = load_model('generator_model.h5')\n",
    "\n",
    "#To create same image, suppy same vector each time\n",
    "# all 0s\n",
    "#vector = asarray([[0. for _ in range(100)]])  #Vector of all zeros\n",
    "\n",
    "#To create random images each time...\n",
    "vector = np.random.randn(100) #Vector of random numbers (creates a column, need to reshape)\n",
    "vector = vector.reshape(1, 100)\n",
    "\n",
    "# generate image\n",
    "X = model.predict(vector)\n",
    "\n",
    "# plot the result\n",
    "pyplot.imshow(X[0, :, :, 0], cmap='gray_r')\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
